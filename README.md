# vLLM Workshop
[![GitHub Pages](https://img.shields.io/badge/docs-GitHub%20Pages-blue)](https://micytao.github.io/vllm-workshop)

ğŸ“š **Hands-on learning powered by [vLLM Playground](https://github.com/micytao/vllm-playground)**

A comprehensive workshop for learning vLLM â€” the high-performance LLM inference engine â€” through practical, hands-on exercises.

## ğŸŒ View the Workshop

**[https://micytao.github.io/vllm-workshop](https://micytao.github.io/vllm-workshop)**

## ğŸ“š What You'll Learn

| Module | Topic | Key Skills |
|--------|-------|------------|
| **Module 1** | Getting Started | Deploy vLLM servers, use chat interface |
| **Module 2** | Structured Outputs | JSON Schema, Regex, Grammar constraints |
| **Module 3** | Tool Calling | Function calling with LLMs |
| **Module 4** | MCP Integration | Agentic AI with human-in-the-loop |
| **Module 5** | Performance Testing | Benchmarking with GuideLLM |

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- GPU with CUDA support (recommended) or CPU
- Podman or Docker

### Install vLLM Playground

```bash
pip install vllm-playground
vllm-playground pull
vllm-playground
```

Then open http://localhost:7860 and follow the workshop modules!

## ğŸ› ï¸ Local Development

To run the documentation site locally:

```bash
# Install MkDocs
pip install mkdocs-material mkdocs-glightbox

# Serve locally
mkdocs serve

# Open http://localhost:8000
```

## ğŸ“– Workshop Structure

```
docs/
â”œâ”€â”€ index.md                    # Welcome page
â”œâ”€â”€ overview.md                 # ACME Corporation narrative
â”œâ”€â”€ details.md                  # Requirements & timing
â”œâ”€â”€ workshop/
â”‚   â”œâ”€â”€ module-01-getting-started.md
â”‚   â”œâ”€â”€ module-02-structured-outputs.md
â”‚   â”œâ”€â”€ module-03-tool-calling.md
â”‚   â”œâ”€â”€ module-04-mcp-integration.md
â”‚   â””â”€â”€ module-05-benchmarking.md
â””â”€â”€ conclusion.md               # Summary & next steps
```

## ğŸ”— Related Projects

- **[vLLM Playground](https://github.com/micytao/vllm-playground)** - The tool used in this workshop
- **[vLLM](https://github.com/vllm-project/vllm)** - High-throughput LLM serving engine
- **[GuideLLM](https://github.com/neuralmagic/guidellm)** - Performance benchmarking

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

## ğŸ“ License

Apache 2.0 License

---

Built with â¤ï¸ for the vLLM community
